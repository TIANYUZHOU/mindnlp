{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore as ms\n",
    "from mindspore import nn\n",
    "from mindnlp.modules import CRF\n",
    "from mindspore import ops\n",
    "\n",
    "class BiLSTM_CRF(nn.Cell):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_tags, padding_idx=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, bidirectional=True, batch_first=True)\n",
    "        self.hidden2tag = nn.Dense(hidden_dim, num_tags, 'he_uniform')\n",
    "        self.crf = CRF(num_tags, batch_first=True)\n",
    "\n",
    "    def construct(self, inputs, seq_length, tags=None):\n",
    "        embeds = self.embedding(inputs)\n",
    "        outputs, _ = self.lstm(embeds, seq_length=seq_length)\n",
    "        feats = self.hidden2tag(outputs)\n",
    "\n",
    "        crf_outs = self.crf(feats, tags, seq_length)\n",
    "        return crf_outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_dim = 16\n",
    "hidden_dim = 32\n",
    "\n",
    "training_data = [(\n",
    "    \"清 华 大 学 坐 落 于 首 都 北 京\".split(),\n",
    "    \"B I I I O O O O O B I\".split()\n",
    "), (\n",
    "    \"重 庆 是 一 个 魔 幻 城 市\".split(),\n",
    "    \"B I O O O O O O O\".split()\n",
    ")]\n",
    "\n",
    "word_to_idx = {}\n",
    "word_to_idx['<pad>'] = 0\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "\n",
    "tag_to_idx = {\"B\": 0, \"I\": 1, \"O\": 2}\n",
    "\n",
    "len(word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = BiLSTM_CRF(len(word_to_idx), embedding_dim, hidden_dim, len(tag_to_idx))\n",
    "optimizer = nn.SGD(model.trainable_params(), learning_rate=0.01, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "\n",
    "train_one_step = nn.TrainOneStepCell(model, optimizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_sequence(seqs, word_to_idx, tag_to_idx):\n",
    "    seq_outputs, label_outputs, seq_length = [], [], []\n",
    "    max_len = max([len(i[0]) for i in seqs])\n",
    "\n",
    "    for seq, tag in seqs:\n",
    "        seq_length.append(len(seq))\n",
    "        idxs = [word_to_idx[w] for w in seq]\n",
    "        labels = [tag_to_idx[t] for t in tag]\n",
    "        idxs.extend([word_to_idx['<pad>'] for i in range(max_len - len(seq))])\n",
    "        labels.extend([tag_to_idx['O'] for i in range(max_len - len(seq))])\n",
    "        seq_outputs.append(idxs)\n",
    "        label_outputs.append(labels)\n",
    "\n",
    "    return ms.Tensor(seq_outputs, ms.int64), \\\n",
    "            ms.Tensor(label_outputs, ms.int64), \\\n",
    "            ms.Tensor(seq_length, ms.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data, label, seq_length = prepare_sequence(training_data, word_to_idx, tag_to_idx)\n",
    "data.shape, label.shape, seq_length.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_one_step.compile(data, seq_length, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "steps = 500\n",
    "with tqdm(total=steps) as t:\n",
    "    for i in range(steps):\n",
    "        loss = train_one_step(data, seq_length, label)\n",
    "        t.set_postfix(loss=loss)\n",
    "        t.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from mindnlp.dataset import CoNLL2000Chunking\n",
    "import mindspore.dataset as ds\n",
    "import mindspore as ms\n",
    "from mindspore.dataset import text,GeneratorDataset\n",
    "\n",
    "dataset_train,dataset_test = CoNLL2000Chunking()\n",
    "\n",
    "columns_to_project = [\"words\", \"chunk_tag\"]\n",
    "\n",
    "dataset_train = dataset_train.project(columns= columns_to_project)\n",
    "dataset_test = dataset_test.project(columns= columns_to_project)\n",
    "\n",
    "input_columns = [\"words\", \"chunk_tag\"]\n",
    "output_columns = [\"text\", \"label\"]\n",
    "\n",
    "dataset_train = dataset_train.rename(input_columns=input_columns, output_columns=output_columns)\n",
    "dataset_test = dataset_test.rename(input_columns=input_columns, output_columns=output_columns)\n",
    "\n",
    "class TmpDataset:\n",
    "    \"\"\" a Dataset for seq_length column \"\"\"\n",
    "    def __init__(self, dataset):\n",
    "        self._dataset = dataset\n",
    "        self._seq_length = []\n",
    "        self._load()\n",
    "\n",
    "    def _load(self):\n",
    "        for data in self._dataset.create_dict_iterator():\n",
    "            self._seq_length.append(len(data[\"text\"]))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self._seq_length[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        print(max(self._seq_length), min(self._seq_length))\n",
    "        return len(self._seq_length)\n",
    "    \n",
    "dataset_train_seq_length =  GeneratorDataset(TmpDataset(dataset_train), [\"seq_length\"],shuffle=False)\n",
    "dataset_test_seq_length =  GeneratorDataset(TmpDataset(dataset_test), [\"seq_length\"],shuffle=False)\n",
    "\n",
    "dataset_train = dataset_train.zip(dataset_train_seq_length)\n",
    "dataset_test = dataset_test.zip(dataset_test_seq_length)\n",
    "\n",
    "# itr = dataset_test.create_dict_iterator()\n",
    "# for i in itr:\n",
    "#     print(i)\n",
    "#     break\n",
    "\n",
    "def tag_idx(tags):\n",
    "    \"\"\" tag_idx \"\"\"\n",
    "    tag_idx_list = []\n",
    "    regex_dic = {\"^B.*\":0, \"^I.*\":1,\"^O.*\":2}\n",
    "    for tag in tags:\n",
    "        for key, value in regex_dic.items():\n",
    "            if re.match(key, tag):\n",
    "                tag_idx_list.append(value)\n",
    "    return tag_idx_list\n",
    "\n",
    "# vocab = dataset_train.build_vocab(columns=[\"text\"],freq_range=None,top_k=None,\n",
    "#                                   special_tokens=[\"<pad>\",\"<unk>\"],special_first=True)\n",
    "\n",
    "vocab = ds.text.Vocab.from_dataset(dataset_train,columns=[\"text\"],freq_range=None,top_k=None,\n",
    "                                   special_tokens=[\"<pad>\",\"<unk>\"],special_first=True)\n",
    "\n",
    "# print(len(vocab.vocab()))\n",
    "# vocab.ids_to_tokens()\n",
    "\n",
    "lookup_op = ds.text.Lookup(vocab, unknown_token=\"<unk>\")\n",
    "pad_text_op = ds.transforms.PadEnd([50],pad_value=vocab.tokens_to_ids('<pad>'))\n",
    "pad_label_op = ds.transforms.PadEnd([50],pad_value=2)\n",
    "type_cast_op = ds.transforms.TypeCast(ms.int64)\n",
    "\n",
    "dataset_train = dataset_train.map(operations=[tag_idx], input_columns=[\"label\"])\n",
    "dataset_train = dataset_train.map(operations=[pad_label_op], input_columns=[\"label\"])\n",
    "dataset_train = dataset_train.map(operations=[lookup_op,pad_text_op], input_columns=[\"text\"])\n",
    "dataset_train = dataset_train.map(operations=[type_cast_op])\n",
    "\n",
    "dataset_test = dataset_test.map(operations=[tag_idx], input_columns=[\"label\"])\n",
    "dataset_test = dataset_test.map(operations=[pad_label_op], input_columns=[\"label\"])\n",
    "dataset_test = dataset_test.map(operations=[lookup_op,pad_text_op], input_columns=[\"text\"])\n",
    "dataset_test = dataset_test.map(operations=[type_cast_op])\n",
    "\n",
    "# dataset_test.bucket_batch_by_length([\"text\",\"label\"],\n",
    "#                                     bucket_boundaries=[10,20,30,40,50],bucket_batch_sizes=[32,16,8,4,2,1])\n",
    "\n",
    "# dataset_test = dataset_test.batch(2)\n",
    "# itr = dataset_test.create_dict_iterator()\n",
    "# for i in itr:\n",
    "#     print(i)\n",
    "#     break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from mindspore.dataset import text\n",
    "from mindspore import nn,ops\n",
    "from mindnlp.dataset import CoNLL2000Chunking, CoNLL2000Chunking_Process\n",
    "from mindnlp.modules import CRF,RNNEncoder\n",
    "from mindnlp.abc import Seq2vecModel\n",
    "from mindnlp.engine.trainer import Trainer\n",
    "\n",
    "dataset_train,dataset_test = CoNLL2000Chunking()\n",
    "\n",
    "vocab = text.Vocab.from_dataset(dataset_train,columns=[\"words\"],freq_range=None,top_k=None,\n",
    "                                   special_tokens=[\"<pad>\",\"<unk>\"],special_first=True)\n",
    "\n",
    "dataset_train = CoNLL2000Chunking_Process(dataset=dataset_train, vocab=vocab, batch_size=64, max_len=80)\n",
    "\n",
    "# data_itr = dataset_train.create_dict_iterator()\n",
    "\n",
    "# print(next(data_itr)[\"text\"])\n",
    "\n",
    "# dataset_train.get_dataset_size()\n",
    "\n",
    "class Head(nn.Cell):\n",
    "    \"\"\" Head for BiLSTM-CRF model \"\"\"\n",
    "    def __init__(self, hidden_dim, num_tags):\n",
    "        super().__init__()\n",
    "        self.hidden2tag = nn.Dense(hidden_dim, num_tags, 'he_uniform')\n",
    "\n",
    "    def construct(self, context):\n",
    "        return self.hidden2tag(context)\n",
    "\n",
    "class BiLSTM_CRF(Seq2vecModel):\n",
    "    \"\"\" BiLSTM-CRF model \"\"\"\n",
    "    def __init__(self, encoder, head, num_tags):\n",
    "        super().__init__(encoder, head)\n",
    "        self.encoder = encoder\n",
    "        self.head = head\n",
    "        self.crf = CRF(num_tags, batch_first=True)\n",
    "\n",
    "    def construct(self, text, seq_length, label=None):\n",
    "        output,_,_ = self.encoder(text)\n",
    "        feats = self.head(output)\n",
    "        res = self.crf(feats, label, seq_length)\n",
    "        return res\n",
    "\n",
    "embedding_dim = 16\n",
    "hidden_dim = 32\n",
    "embedding = nn.Embedding(vocab_size=len(vocab.vocab()), embedding_size=embedding_dim, padding_idx=0)\n",
    "lstm_layer = nn.LSTM(embedding_dim, hidden_dim // 2, bidirectional=True, batch_first=True)\n",
    "encoder = RNNEncoder(embedding, lstm_layer)\n",
    "head = Head(hidden_dim, 3)\n",
    "net = BiLSTM_CRF(encoder, head, 3)\n",
    "\n",
    "optimizer = nn.SGD(net.trainable_params(), learning_rate=0.01, weight_decay=1e-4)\n",
    "\n",
    "grad_fn = ops.value_and_grad(net, None, optimizer.parameters)\n",
    "\n",
    "def train_step(seqs, seq_length, label):\n",
    "    \"\"\" train_step \"\"\"\n",
    "    loss, grads = grad_fn(seqs, seq_length, label)\n",
    "    loss = ops.depend(loss, optimizer(grads))\n",
    "    return loss\n",
    "\n",
    "for batch, (data, label, seq_length) in enumerate(dataset_train.create_tuple_iterator()):\n",
    "    loss = train_step(data, seq_length , label)\n",
    "\n",
    "print(\">>>>>>开始训练<<<<<<\")\n",
    "steps = 140\n",
    "with tqdm(total=steps) as t:\n",
    "    for i in range(steps):\n",
    "        for batch, (data, label, seq_length) in enumerate(dataset_train.create_tuple_iterator()):\n",
    "            loss = train_step(data, seq_length ,label)\n",
    "            t.set_postfix(loss=loss)\n",
    "            t.update(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('mindspore')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "819093ef265e896eb27d329f646d5e83f4eb3fe0ec75dbd5037be6a5cf3f0a63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
